{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "008d3ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from: /mnt/d/code/accelerated_features/modules/../weights/xfeat_perm_steer.pth\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset # Direct import is fine\n",
    "\n",
    "from gluefactory.settings import DATA_PATH # Assuming this is where datasets are stored\n",
    "from gluefactory.utils.image import ImagePreprocessor, load_image # Use Glue Factory's image utils\n",
    "from gluefactory.utils.equirectangular_utils import equirectangular_to_dicemap # Import equirectangular utils\n",
    "from gluefactory.utils.spherical_utils import standard_spherical_to_pixel, rotate_image, rotate_keypoints, spherical_to_cartesian # Import spherical utils\n",
    "from gluefactory.utils.xfeat_utils import generate_keypoints # Import xfeat utils\n",
    "from gluefactory.datasets.base_dataset import BaseDataset # Crucial import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4848290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing parameters\n",
    "BATCH_SIZE = 8  # Adjust based on your GPU memory\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "IMG_DIR = \"/mnt/d/code/glue-factory/data/pretraining/images\"\n",
    "OUTPUT_DIR = \"/mnt/d/code/glue-factory/data/pretraining/pairs_batched\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004d7251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotation ranges\n",
    "aug_rot_yaw_range =  180.0 # degrees\n",
    "aug_rot_pitch_range = 90.0 # degrees\n",
    "aug_rot_roll_range = 180.0 # degrees\n",
    "\n",
    "\n",
    "# Photometric augmentation parameters\n",
    "photo_aug_prob = 0.8  # Probability of applying photometric augmentations\n",
    "photo_brightness_range = 0.3  # e.g., adds a value in [-0.3, 0.3]\n",
    "photo_contrast_range = (0.5, 1.5) # Multiplies by a value in [0.5, 1.5]\n",
    "photo_gaussian_noise_std_range = (0.0, 0.05) # Std dev of noise\n",
    "\n",
    "# Keypoints paramters\n",
    "angle_threshold_degrees = 0.5\n",
    "num_keypoints = 2048\n",
    "\n",
    "# Image per pair\n",
    "num_pairs_per_image= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6efac3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_photometric_augmentations(img_np):\n",
    "    \"\"\"Applies a chain of random photometric augmentations to a numpy image.\"\"\"\n",
    "    \n",
    "    # Apply augmentations with a certain probability\n",
    "    if np.random.rand() > photo_aug_prob:\n",
    "        return img_np\n",
    "    \n",
    "    # --- 1. Brightness & Contrast Adjustment ---\n",
    "    # A common and effective way is alpha-beta correction: g(x) = alpha*f(x) + beta\n",
    "    # alpha controls contrast, beta controls brightness.\n",
    "    \n",
    "    # Randomly sample contrast factor (alpha)\n",
    "    alpha = np.random.uniform(photo_contrast_range[0], photo_contrast_range[1])\n",
    "    # Randomly sample brightness factor (beta)\n",
    "    beta = np.random.uniform(-photo_brightness_range, photo_brightness_range)\n",
    "    \n",
    "    # Apply the transformation\n",
    "    img_aug = img_np * alpha + beta\n",
    "    \n",
    "    # We must clip the values to be in the valid [0, 1] range\n",
    "    img_aug = np.clip(img_aug, 0.0, 1.0)\n",
    "\n",
    "    # --- 2. Gaussian Noise ---\n",
    "    # Add random noise to simulate sensor noise\n",
    "    \n",
    "    # Randomly sample the standard deviation of the noise\n",
    "    noise_std = np.random.uniform(photo_gaussian_noise_std_range[0], photo_gaussian_noise_std_range[1])\n",
    "    if noise_std > 0:\n",
    "        # Generate noise with the same shape as the image\n",
    "        gaussian_noise = np.random.normal(loc=0.0, scale=noise_std, size=img_aug.shape)\n",
    "        # Add noise to the image\n",
    "        img_aug = img_aug + gaussian_noise\n",
    "        \n",
    "        # Clip again to ensure values are valid\n",
    "        img_aug = np.clip(img_aug, 0.0, 1.0)\n",
    "        \n",
    "    # --- 3. (Optional but Recommended) Gaussian Blur ---\n",
    "    # Simulates motion blur or out-of-focus cameras\n",
    "    if np.random.rand() < 0.5: # Apply blur with 50% probability\n",
    "        # Kernel size must be odd\n",
    "        sigma = np.random.uniform(0.5, 1.5)\n",
    "        ksize = int(2 * np.ceil(2 * sigma) + 1)\n",
    "        img_aug = cv2.GaussianBlur(img_aug, (ksize, ksize), sigmaX=sigma)\n",
    "        \n",
    "    return img_aug\n",
    "\n",
    "def _generate_groundtruth_correspondences(view1, view2, angle_threshold_degrees=3.0):\n",
    "    \"\"\"Generates ground truth match between two views keypoints\"\"\"\n",
    "\n",
    "    # Convert keypoints from view1 to of view2 and then compare them to a threshould\n",
    "    # using information of image rotation from org_view -> view1 and org_view -> view2\n",
    "\n",
    "    keypoints1 = view1[1][0] # Xfeat Keypoints from view1\n",
    "    keypoints2 = view2[1][0] # Xfeat keypoints from view2\n",
    "\n",
    "    N = len(keypoints1)\n",
    "    M = len(keypoints2)\n",
    "\n",
    "    # Initialize the outputs\n",
    "    matches_view1_to_view2 = np.full(N, -1, dtype=int)\n",
    "    matches_view2_to_view1 = np.full(M, -1, dtype=int)\n",
    "    matches_pairs = np.empty((0, 2), dtype=int)\n",
    "\n",
    "    if N == 0 or M == 0: # In case no keypoints detected\n",
    "        return {\n",
    "            'matches': matches_pairs,\n",
    "            'gt_matches0': matches_view1_to_view2,\n",
    "            'gt_matches1': matches_view2_to_view1\n",
    "        }\n",
    "\n",
    "    yaw1, pitch1, roll1 = view1[2] # img rotationg angles from org_view -> view1\n",
    "    yaw2, pitch2, roll2 = view2[2] # img rotationg angles from org_view -> view2\n",
    "\n",
    "    # 1. inverse rotation as moving keypoints from view1 -> org_view        \n",
    "    kpts1_to_kpts = rotate_keypoints(keypoints1, yaw1, pitch1, roll1, inverse=True)\n",
    "    # direct rotaion as moving keypoints from org_view -> view2\n",
    "    kpts_to_kpts2 = rotate_keypoints(kpts1_to_kpts, yaw2, pitch2, roll2)\n",
    "\n",
    "    # 2. Convert to 3D cartesian coordinate for distance calculatoin\n",
    "    # These are unit vectors on a sphere\n",
    "    kpts_to_kpts2_xyz = spherical_to_cartesian(kpts_to_kpts2)\n",
    "    keypoints2_xyz = spherical_to_cartesian(keypoints2)\n",
    "\n",
    "    # 3. Create a pairwise distance matrix (N x M)\n",
    "    # we use dot production. Since unit vectors, dot(a, b) = cos(angle) -> angle = arccos(dot(a, b))\n",
    "    # The einsum computes the dot product for every pair.\n",
    "    similarity_matrix = np.einsum('ik,jk->ij', kpts_to_kpts2_xyz, keypoints2_xyz)\n",
    "\n",
    "    # Clip the values to avoid numerical errors with arccos\n",
    "    similarity_matrix = np.clip(similarity_matrix, -1.0, 1.0)\n",
    "\n",
    "    # The distance is the angle in degrees\n",
    "    angular_distance_matrix = np.rad2deg(np.arccos(similarity_matrix))\n",
    "\n",
    "    # 4. Search for nearest neighbors in both directions\n",
    "    # For each kpt in view1, find its nearest neighbour in view2\n",
    "    best_match_for_kpt1 = np.argmin(angular_distance_matrix, axis=1) # Shape: (N,)\n",
    "    min_distances_for_kpt1 = np.min(angular_distance_matrix, axis=1)\n",
    "\n",
    "    # For each kpt in view2, find its nearest neighbour in view1\n",
    "    best_match_for_kpt2 = np.argmin(angular_distance_matrix, axis=0) # Shape: (M,)\n",
    "    min_distances_for_kpt2 = np.min(angular_distance_matrix, axis=0)\n",
    "\n",
    "    # 5. Apply the tolerance threshold\n",
    "    # Find all keypoints in view1 that have a match within the threshold\n",
    "    valid_matches_mask = min_distances_for_kpt1 < angle_threshold_degrees\n",
    "\n",
    "    # 6. Enforce mutual consistency (the \"Mututual Nearest Neighbor\" check --- Lighglue Paper)\n",
    "    # We create an index array [0, 1, 2, ..., N-1]\n",
    "    kpt1_indices = np.arange(N)\n",
    "\n",
    "    # We check if the best match for kpt1's best matach is kpt1 it    # Example: If kpt1[i]'s best match is kpt2[j], we then check if kpt2[j]'s best match is kpt1[i].\n",
    "    mutual_matches_mask = (best_match_for_kpt2[best_match_for_kpt1] == kpt1_indices)\n",
    "\n",
    "    # The final mask combines the distance threshold and the mutual check\n",
    "    final_mask = valid_matches_mask & mutual_matches_mask\n",
    "\n",
    "    # Create the final list of matches\n",
    "    # This will be an array of shape (num_matches, 2), where each row is (idx_kpt1, idx_kpt2)\n",
    "    matches = np.stack([kpt1_indices[final_mask], best_match_for_kpt1[final_mask]], axis=1)\n",
    "\n",
    "    # Get the indices of keypoints in view1 that have a valid mutual match\n",
    "    matched_kpt1_indices = kpt1_indices[final_mask]\n",
    "\n",
    "    # Get the corresponding matched keypoint indices in view2\n",
    "    matched_kpt2_indices = best_match_for_kpt1[final_mask]\n",
    "\n",
    "    # 1. Populate the original [num_matches, 2] array\n",
    "    matches_pairs = np.stack([matched_kpt1_indices, matched_kpt2_indices], axis=1)\n",
    "\n",
    "    # 2. Populate the gt_matches0 array\n",
    "    matches_view1_to_view2[matched_kpt1_indices] = matched_kpt2_indices\n",
    "\n",
    "    # 3. Populate the gt_matches1 array (the inverse mapping)\n",
    "    matches_view2_to_view1[matched_kpt2_indices] = matched_kpt1_indices\n",
    "\n",
    "    return {\n",
    "        'matches': matches_pairs,\n",
    "        'gt_matches0': matches_view1_to_view2,\n",
    "        'gt_matches1': matches_view2_to_view1\n",
    "    }\n",
    "\n",
    "def _generate_views(img_raw):\n",
    "    \"\"\"Generates two views from the same image with random rotations and alterations.\"\"\"\n",
    "\n",
    "    # Randomly generate yaw, pitch, roll angles within the specified ranges\n",
    "    # These angles are in degrees, and will be used to rotate the image\n",
    "    yaw = np.random.uniform(-aug_rot_yaw_range, aug_rot_yaw_range)\n",
    "    pitch = np.random.uniform(-aug_rot_pitch_range, aug_rot_pitch_range)\n",
    "    roll = np.random.uniform(-aug_rot_roll_range, aug_rot_roll_range)\n",
    "\n",
    "    # Rotate the image\n",
    "    img_rotated = rotate_image(img_raw, yaw, pitch, roll)\n",
    "\n",
    "    img_augmented = _apply_photometric_augmentations(img_rotated)\n",
    "\n",
    "    # Convert the rotated image to dicemap\n",
    "    img_dicemap = equirectangular_to_dicemap(img_augmented)\n",
    "\n",
    "    # Generate keypoints, descriptors and scores from xfeat\n",
    "    # Takes dicemap image but internally convert xfeat output back to equirectangular format\n",
    "    kpt_data = generate_keypoints(img_dicemap, num_keypoints=num_keypoints) # tuple: (keypointCoords, keypointDescriptors, keypointScores)\n",
    "\n",
    "    # Convert spherical keypoints (phi, theta) to equirectangular pixel coordinates\n",
    "    # kpt_pixels = standard_spherical_to_pixel(kpt_data[0], img_augmented.shape[1], img_augmented.shape[0])\n",
    "\n",
    "    # Plot the augmented image and overlay keypoints\n",
    "    # plt.figure(figsize=(12, 6))\n",
    "    # plt.imshow(img_augmented)\n",
    "    # plt.scatter(kpt_pixels[:, 0], kpt_pixels[:, 1], s=8, c='r', marker='o')\n",
    "    # plt.title(\"Augmented Image with Keypoints\")\n",
    "    # plt.axis('off')\n",
    "    # plt.show()\n",
    "\n",
    "    # Convert rotated image back to tensor and convert it to (C, H, W)\n",
    "    augmented_image = torch.tensor(img_augmented.transpose(2, 0, 1))  # Convert back to (C, H, W)\n",
    "\n",
    "    return (augmented_image, kpt_data, (yaw, pitch, roll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909d103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/mnt/d/code/glue-factory/data/pretraining/images\"\n",
    "image_files = os.listdir(image_path)\n",
    "\n",
    "real = ['berlinStreet', 'church', 'corridors', 'meetingRoom1', 'meetingRoom2', 'stadium', 'townSquare', 'trainStation', 'uni' ]\n",
    "\n",
    "for img in tqdm(image_files, desc=\"Processing Images\", leave=True, colour='green'):\n",
    "    img_name = img.split('_')[0]\n",
    "    if img_name not in real:\n",
    "        continue\n",
    "    else:\n",
    "        # Read image\n",
    "        img_raw_torch = load_image(os.path.join(image_path, img))\n",
    "\n",
    "        for i in range(num_pairs_per_image):\n",
    "            # Generate Views\n",
    "            view0 = _generate_views(img_raw_torch)\n",
    "            view1 = _generate_views(img_raw_torch)\n",
    "\n",
    "            # Generate groundtruth matches\n",
    "            gt_data = _generate_groundtruth_correspondences(view0, view1, angle_threshold_degrees=angle_threshold_degrees)\n",
    "\n",
    "            # Convert numpy arrays from gt_data to tensors, ensuring correct dtypes\n",
    "            matches = torch.from_numpy(gt_data['matches']).long()\n",
    "            gt_matches0 = torch.from_numpy(gt_data['gt_matches0']).long()\n",
    "            gt_matches1 = torch.from_numpy(gt_data['gt_matches1']).long()\n",
    "\n",
    "            name = f\"{img.split('.')[0]}_{i}\"\n",
    "            # Save to npz file\n",
    "            np.savez(\n",
    "                f\"/mnt/d/code/glue-factory/data/pretraining/pairs/{name}.npz\",\n",
    "                # image0=view0[0],\n",
    "                \n",
    "                keypoints0=view0[1][0],\n",
    "                descriptors0=view0[1][1],\n",
    "                scores0=view0[1][2],\n",
    "                image_size0=torch.tensor(view0[0].shape[-2:][::-1]),\n",
    "                yaw_pitch_roll_0=view0[2],\n",
    "                # image1=view1[0],\n",
    "                keypoints1=view1[1][0],\n",
    "                descriptors1=view1[1][1],\n",
    "                scores1=view1[1][2],\n",
    "                image_size1=torch.tensor(view1[0].shape[-2:][::-1]),\n",
    "                yaw_pitch_roll_1=view1[2],\n",
    "                matches=matches,           # Original [num_matches, 2] format, if needed elsewhere\n",
    "                gt_matches0=gt_matches0,\n",
    "                gt_matches1=gt_matches1,\n",
    "                name=f\"{name}\" # Base image used for alterations\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gluefactory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
