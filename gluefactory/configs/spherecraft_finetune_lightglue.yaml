# @package _global_

# This configuration is for FINE-TUNING LightGlue on the SphereCraft dataset.

train:
  epochs: 50  # Fewer epochs are often needed for fine-tuning
  lr: 0.00005 # Use a smaller learning rate for fine-tuning
  optimizer: 'adamw'
  log_every_iter: 100
  eval_every_iter: 1000
  save_every_iter: 1000
  best_key: 'loss/total'
  clip_grad: 1.0
  lr_schedule:
    type: 'warmup_cosine' # Custom type we will implement
    warmup_steps: 500       # Number of warmup steps
    # total_steps will be calculated automatically
  seed: 42
  keep_last_checkpoints: 5
  
  # --- KEY CHANGE FOR FINE-TUNING ---
  # Specify the experiment whose weights we want to load.
  # If it's a file path and not an experiment name, we'll handle it in the command.
  # For now, we point to the model we will use.
  load_experiment: "spherecraft_pretrain_lightglue_run3"

  # Ensure the dataset module is discoverable
  submodules: ['gluefactory.datasets.spherecraft']

# --- KEY CHANGE: Define the model as LightGlue ---
model:
  name: lightglue
  features: 'xfeat' # Must match the features the model was trained on
  descriptor_dim: 96
  n_layers: 6
  num_heads: 1
  flash: False
  width_confidence: -1
  filter_threshold: 0.1

# Define the dataset (this section remains the same)
data:
  name: spherecraft
  
  # --- SphereCraft-specific parameters ---
  data_dir: 'finetuning'
  pair_subdir: 'finetuning_pairs_1'
  keypoints_detector: 'xfeat'
  
  # Batch sizes and workers
  train_batch_size: 16
  num_workers: 4