# @package _global_

# This configuration is for pretraining LightGlue on the SphereCraft dataset.

train:
  epochs: 40 # Reduced epochs for pretraining
  lr: 0.0005 # Use a smaller learning rate for pretraining
  optimizer: 'adamw'
  log_every_iter: 100
  eval_every_iter: 1000
  save_every_iter: 1000
  best_key: 'loss/total'
  clip_grad: 1.0
  lr_schedule:
    type: 'warmup_cosine' # Custom type we will implement
    warmup_steps: 500       # Number of warmup steps
    # total_steps will be calculated automatically
  seed: 42
  keep_last_checkpoints: 5

  # Ensure the dataset module is discoverable
  submodules: ['gluefactory.datasets.spherecraft']

# --- KEY CHANGE: Define the model as LightGlue ---
model:
  name: lightglue
  features: 'xfeat' # Must match the features the model was trained on
  descriptor_dim: 96
  n_layers: 6
  num_heads: 1
  flash: False
  width_confidence: -1
  filter_threshold: 0.1

# Define the dataset (this section remains the same)
data:
  name: spherecraft
  
  # --- SphereCraft-specific parameters ---
  data_dir: 'pretraining'
  pair_subdir: 'pairs'
  keypoints_detector: 'xfeat'
  
  # Batch sizes and workers
  train_batch_size: 320
  num_workers: 16